<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Trial &amp; the mock library
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Trial%20%26%20the%20mock%20library&In-Reply-To=%3CE5207BFC-CD93-4F35-9C84-1EA7144D782A%40twistedmatrix.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="059715.html">
   <LINK REL="Next"  HREF="059717.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Trial &amp; the mock library</H1>
    <B>Glyph</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Trial%20%26%20the%20mock%20library&In-Reply-To=%3CE5207BFC-CD93-4F35-9C84-1EA7144D782A%40twistedmatrix.com%3E"
       TITLE="[Twisted-Python] Trial &amp; the mock library">glyph at twistedmatrix.com
       </A><BR>
    <I>Fri Jul 26 22:26:24 MDT 2013</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="059715.html">[Twisted-Python] Trial &amp; the mock library
</A></li>
        <LI>Next message (by thread): <A HREF="059717.html">[Twisted-Python] Trial &amp; the mock library
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#59716">[ date ]</a>
              <a href="thread.html#59716">[ thread ]</a>
              <a href="subject.html#59716">[ subject ]</a>
              <a href="author.html#59716">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>
On Jul 26, 2013, at 7:12 AM, <A HREF="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">exarkun at twistedmatrix.com</A> wrote:

&gt;<i> To address this problem, I suggest you get into the habit of watching your unit tests fail in the expected way before you make the necessary implementation changes to make them pass.
</I>&gt;<i> 
</I>&gt;<i> This is only one of an unlimited number of ways your unit tests can be buggy.  It might be tempting to try to fix the test runner to prevent you from ever falling into this trap again - and who knows, it might even be a good idea.
</I>&gt;<i> However, if you run your tests and see them fail in the way you expected them to fail before you write the code that makes them pass, then you will be sure to avoid the many, many, many *other* pitfalls that have nothing to do with accidentally returning the wrong object.
</I>&gt;<i> 
</I>&gt;<i> This is just one of the attractions of test-driven development for me.
</I>
On a more serious note than our previous digression, perhaps *this* is the thing we should be modifying Trial to support.

The vast majority of Twisted committers do development this way - or at least aspire to, most of the time - but to someone new to automated testing, it's not entirely clear how you're supposed to use something like Trial, or how important it is that you see the tests fail first.

Perhaps if trial had a bit more of a memory of things that happened between test runs it would be useful.  For example, a mode where you could tell it what you're working on, and you could just re-run the same thing and you'd only get a 'success' when you went back and forth between red and green.

Here's a silly little narrative about how one might use such a thing:

$ tribulation begin myproject
Beginning a time of turmoil for python package 'myproject', in './myproject/'.
myproject.test_1
  Case1
    test_1 ...                                                             [OK]

-------------------------------------------------------------------------------
Ran 2 tests in 0.033s

PROCEED (successes=1) - All tests passing, an auspicious beginning. Now write a failing test.
$ tribulation continue
myproject.test_1
  Case1
    test_1 ...                                                             [OK]
myproject.test_2
  Case2
    test_2 ...                                                             [OK]

-------------------------------------------------------------------------------
Ran 2 tests in 0.033s

AGAIN (successes=2) - a test should have failed.
# oops, 'test_2' was just 'pass'... let me fix that
$ tribulation continue
$ tribulation begin myproject
Beginning a time of turmoil for python package 'myproject', in './myproject/'.
myproject.test_1
  Case1
    test_1 ...                                                             [OK]
myproject.test_2
  Case2
    test_2 ...                                                           [FAIL]

-------------------------------------------------------------------------------
Ran 2 tests in 0.450s

PROCEED (successes=2) - we are working on myproject.Case2.test_2 now.
$ tribulation continue
myproject.test_2
  Case2
    test_2 ...                                                           [FAIL]

-------------------------------------------------------------------------------
Ran 1 tests in 0.020s
AGAIN (successes=2) - you should have made the test pass.
$ tribulation continue
myproject.test_2
  Case2
    test_2 ...                                                             [OK]

-------------------------------------------------------------------------------
Ran 1 tests in 0.01s
PROCEED (successes=1) - myproject.Case2.test_2 works now, let's make sure nothing else broke.
$ tribulation continue
myproject.test_1
  Case1
    test_1 ...                                                             [OK]
myproject.test_2
  Case2
    test_2 ...                                                             [OK]

-------------------------------------------------------------------------------
Ran 2 tests in 0.033s
PROCEED (successes=2) - no regressions, find the next thing to work on
$ tribulation conclude
You have received one billion points, congratulations you have defeated software.

Does this seem like it might be a useful feature for someone to work on?  Not shown here is the part that when you do introduce a regression, it runs just the tests that failed until you fix all of them, then goes back up the suite until it reaches the top and you move on to the next thing...

-glyph

-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;/pipermail/twisted-python/attachments/20130726/cdee24d2/attachment-0002.html&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="059715.html">[Twisted-Python] Trial &amp; the mock library
</A></li>
	<LI>Next message (by thread): <A HREF="059717.html">[Twisted-Python] Trial &amp; the mock library
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#59716">[ date ]</a>
              <a href="thread.html#59716">[ thread ]</a>
              <a href="subject.html#59716">[ subject ]</a>
              <a href="author.html#59716">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
