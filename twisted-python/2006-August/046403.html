<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Multicast XMLRPC
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44F05249.2060803%40imperial.ac.uk%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="046402.html">
   <LINK REL="Next"  HREF="046404.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Multicast XMLRPC</H1>
    <B>Phil Mayers</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44F05249.2060803%40imperial.ac.uk%3E"
       TITLE="[Twisted-Python] Multicast XMLRPC">p.mayers at imperial.ac.uk
       </A><BR>
    <I>Sat Aug 26 07:53:13 MDT 2006</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="046402.html">[Twisted-Python] Multicast XMLRPC
</A></li>
        <LI>Next message (by thread): <A HREF="046404.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46403">[ date ]</a>
              <a href="thread.html#46403">[ thread ]</a>
              <a href="subject.html#46403">[ subject ]</a>
              <a href="author.html#46403">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Chaz. wrote:

&gt;<i> 
</I>&gt;<i> I started out using Spread some time ago (more than 2 years ago). The 
</I>&gt;<i> implementation was limited to a hundred or so nodes (that is in the 
</I>&gt;<i> notes on the spread implementation). Secondly it isn't quite so 
</I>&gt;<i> lightweight as you think (I've measured the performance).
</I>&gt;<i> 
</I>&gt;<i> It is a very nice system but when it gets to 1000s of machines very 
</I>&gt;<i> little work has been done on solving many of the problems. My research 
</I>&gt;<i> on it goes back almost a decade starting out with Horus.
</I>
I must admit to not having attempted to scale it that far, but I was 
under the impression that only the more expensive delivery modes were 
that costly. But by the sounds of it, you don't need me to tell you that.

&gt;<i> 
</I>&gt;<i> Actually I am part of the IRTF group on P2P, E2E and SAM. I know the 
</I>&gt;<i> approaches they are being tossed about. I have tried to implement some 
</I>&gt;<i> of them. I just am not of the opinion that smart people can't find 
</I>&gt;<i> solutions to tough problems.
</I>

Ok, in which case my apologies. My reading of your posts had lead me to 
believe, incorrectly, you may not be familiar with the various issues. 
In that case, you can (should) disregard most of it.

&gt;<i> 
</I>&gt;<i> Is multicast or broadcast the right way? I don't know, but I do know 
</I>&gt;<i> that without trying we will never know. Having been part of the IETF 
</I>
It's clearly right for some things - I'm just not sure how much 
bi-directional distribution would be helped by it, since you've got at 
some point to get the replies back.

&gt;<i> community for a lot of years (I was part of the group that worked on 
</I>&gt;<i> SNMP v1 and the WinSock standard), I know that when the &quot;pedal meets the 
</I>&gt;<i> metal&quot; sometimes you discover interesting things.
</I>
I didn't realise winsock went near the IETF. You learn something new 
every day.

&gt;&gt;<i>
</I>&gt;&gt;<i> Knuth and his comments on early optimisation apply here. Have you 
</I>&gt;&gt;<i> tried it? You might be surprised.
</I>&gt;&gt;<i>
</I>&gt;<i> 
</I>&gt;<i> I am sorry to say I don't know the paper or research you are referring 
</I>&gt;<i> to. Can you point me to some references?
</I>
Sorry, it's a phrase from Donald Knuth's (excellent) three-volume 
programming book, &quot;The Art of Computer Programming&quot;. Highly recommended.

&gt;<i> 
</I>&gt;<i> Thanks for the information. This is what makes me think that I want 
</I>&gt;<i> something based on UDP and not TCP! And if I can do RMT (or some variant 
</I>&gt;<i> of it) I might be able to get better performance. But, as I said it is 
</I>&gt;<i> the nice thing about not having someone telling me I need to get a 
</I>&gt;<i> product out the door tomorrow! I have time to experiment and learn.
</I>
When I wrote my reply I hadn't seen your comment on the app being 
distributed storage.

&gt;&gt;<i> How many calls per second are you doing, and approximately what volume 
</I>&gt;&gt;<i> of data will each call exchange?
</I>&gt;&gt;<i>
</I>&gt;<i> This is information I can't provide since the system I have designing 
</I>&gt;<i> has no equivalent in the marketplace today (either commercial or open 
</I>&gt;<i> source). All I know is that the first version of the system I built - 
</I>&gt;<i> using C/C++ and a traditional architecture (a few dozens of machines) 
</I>&gt;<i> was able to handle 200 transactions/minute (using SOAP). While there 
</I>&gt;<i> were some &quot;short messages&quot; (less than an normal MTU), I had quite a few 
</I>&gt;<i> that topped out 50K bytes and some up to 100Mbytes.
</I>
Oh. In which case more or less everything I wrote is useless!

&gt;&gt;<i>
</I>&gt;&gt;<i> Successful transmission is really the easy bit for multicast. There is 
</I>&gt;&gt;<i> IGMP snooping, IGMP querier misbehaviour, loss of forwarding on an 
</I>&gt;&gt;<i> upstream IGP flap, flooding issues due to global MSDP issues, and so 
</I>&gt;&gt;<i> forth.
</I>&gt;&gt;<i>
</I>&gt;<i> 
</I>&gt;<i> I agree about the successful transmission. You've lost me on the IGMP 
</I>&gt;<i> part. Can you elaborate as to your thoughts?
</I>
Well, my experience of large multicast IPv4 networks is that short 
interruptions in multicast connectivity are not uncommon. There are a 
number of reasons for this, which can be broadly broken down into 1st 
hop and subsequent hop issues.

Basically, in a routed-multicast environment, I've seen the subnet IGMP 
querier (normally the gateway) get pre-empted by badly configured or 
plain broken OS stacks (e.g. someone running Linux with the IGMPv3 early 
patches). I've also seen confusion for highly-available subnets (e.g. 
VRRPed networks) where the IGMP querier and the multicast designated 
forwarder are different. This can cause issues with the IGMP snooping on 
the downstream layer2 switches when the DF is no longer on the path 
which the layer2 snooping builds.

You also get issues with upstream changes in the unicast routing 
topology affecting PIM.

Most of these are only issues with routed multicast. Subnet-local is a 
lot simpler, though you do still need an IGMP querier and switches with 
IGMP snooping.

&gt;<i> 
</I>&gt;<i> I do know I need EXACTLY-ONCE semantics but how and where I implement 
</I>&gt;<i> them is the unknown. When you use TCP you assume the network provides 
</I>&gt;<i> the bulk of the solution. I have been thinking that if I use a less 
</I>&gt;<i> reliable network - one with low overhead - that I can provide the server 
</I>&gt;<i> part to do the EXACTLY-ONCE piece.
</I>&gt;<i> 
</I>&gt;<i> As to why I need EXACTLY-ONCE, well if I have to store something I know 
</I>&gt;<i> I absolutely need to store it. I can't be in the position that I don't 
</I>&gt;<i> know it has been stored - it must be there.
</I>&gt;<i> 
</I>&gt;<i> Thanks for the great remarks....I look forward to reading more.
</I>
This makes a lot more sense now I know it's storage related.

You're right, this is a tricky and uncommon problem.

Let me see if I've got this right:

You're building some kind of distributed storage service. Clients will 
access the storage by a &quot;normal&quot; protocol to one of the nodes. Reads 
from the store are relatively easy, but writes to the store will need to 
be distributed to all or a subset of the nodes. Obviously you'll have a 
mix of lots of small writes and some very large writes.

Hmm.

Are you envisioning that you might have &gt;1 storage set on the nodes, and 
using a different multicast group per storage set to build optimal 
distribution?

You might be able to perform some tricks depending on whether this 
service provides block- or filesystem-level semantics. If it's the 
latter, you could import some techniques from the distributed version 
control arena - broadly speaking, node+version number each file and 
&quot;broadcast&quot; (in the application sense) just the file + 
newnode+newversion to the other store nodes, and have them lock the 
local copy and initiate a pull from the updated node.

For block-level storage, that's going to be a lot harder.

For the multicast, something like NORM, which as you probably know is 
basically forward-error-corrected transmit channel with 
receiver-triggered re-transmits, would probably work. An implementation 
would likely be non-trivial, but a fascinating project.


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="046402.html">[Twisted-Python] Multicast XMLRPC
</A></li>
	<LI>Next message (by thread): <A HREF="046404.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46403">[ date ]</a>
              <a href="thread.html#46403">[ thread ]</a>
              <a href="subject.html#46403">[ subject ]</a>
              <a href="author.html#46403">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
