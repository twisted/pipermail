<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Multicast XMLRPC
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44F047E1.4010809%40gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="046400.html">
   <LINK REL="Next"  HREF="046402.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Multicast XMLRPC</H1>
    <B>Chaz.</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44F047E1.4010809%40gmail.com%3E"
       TITLE="[Twisted-Python] Multicast XMLRPC">eprparadocs at gmail.com
       </A><BR>
    <I>Sat Aug 26 07:08:49 MDT 2006</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="046400.html">[Twisted-Python] Multicast XMLRPC
</A></li>
        <LI>Next message (by thread): <A HREF="046402.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46401">[ date ]</a>
              <a href="thread.html#46401">[ thread ]</a>
              <a href="subject.html#46401">[ subject ]</a>
              <a href="author.html#46401">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Phil Mayers wrote:
&gt;<i> Chaz. wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Now let me address the issue of TCP. It is a pretty heavy protocol to 
</I>&gt;&gt;<i> use. It takes a lot of resources on the sender and target and can take 
</I>&gt;&gt;<i> some time to establish a connection. Opening a 1000 or more sockets 
</I>&gt;&gt;<i> consumes a lot of resources in the underlying OS and in the Twisted 
</I>&gt;&gt;<i> client!
</I>&gt;<i> 
</I>&gt;<i> People keep trying to help you, and you keep repeating yourself. From 
</I>&gt;<i> what I can gather:
</I>&gt;<i> 
</I>&gt;<i> You *need* a relatively lightweight group communication method. My 
</I>&gt;<i> advice would be to investigate a message bus - see recent posts on this 
</I>&gt;<i> mailing list. &quot;Spread&quot; at www.spread.org and ActiveMQ (via the simple 
</I>&gt;<i> text-over-tcp-based STOMP protocol). Reports are that both can (under 
</I>&gt;<i> the right conditions) execute many thousands of group messages per second.
</I>&gt;<i> 
</I>
I started out using Spread some time ago (more than 2 years ago). The 
implementation was limited to a hundred or so nodes (that is in the 
notes on the spread implementation). Secondly it isn't quite so 
lightweight as you think (I've measured the performance).

It is a very nice system but when it gets to 1000s of machines very 
little work has been done on solving many of the problems. My research 
on it goes back almost a decade starting out with Horus.

&gt;<i> Failing that, Glyph has hinted at another approach. You could elect a 
</I>&gt;<i> small number (~1%) of your nodes as &quot;proxies&quot; so that as well as being 
</I>&gt;<i> clients, they act as intermediaries for messages. This is a simple form 
</I>&gt;<i> of overlay network, which you also stated you didn't want to use - lord 
</I>&gt;<i> knows why. People use these techniques for a reason - they work.
</I>&gt;<i> 
</I>
I know about overlay networks, gossip networks, etc. I have used both 
and would prefer something simpler. That is the reason for my pushing on 
this group - to see what ideas people might have. I appreciate Glyph's 
comments and perspectives - very refreshing - in contrast to the many I 
have gotten.

&gt;<i> You *want* (have decided you want) a reliable multicast protocol over 
</I>&gt;<i> which you'll layer a simple RPC protocol. RMT (reliable multicast 
</I>&gt;<i> transport) is as yet an unsolved problem. It is VERY VERY hard. None 
</I>&gt;<i> exist for Twisted, to the best of my knowledge. I would be willing to 
</I>&gt;<i> bet money that, for &quot;thousands&quot; of nodes, the overhead of implementing 
</I>&gt;<i> such a protocol (in Python, one presumes) would exceed the overhead of 
</I>&gt;<i> just using TCP. If you had said &quot;hundreds of thousands&quot; of nodes, well, 
</I>&gt;<i> that would be different.
</I>&gt;<i> 
</I>&gt;<i> If you want to knock an RMT up based on the assumption you won't drop 
</I>&gt;<i> packets, then be my guest, but I would suggest that if you *really* 
</I>&gt;<i> believe multicast is that reliable, then your experience of IP multicast 
</I>&gt;<i> networks has been a lot more rosy than mine, and I run a very large one.
</I>&gt;<i> 
</I>&gt;<i> &quot;reliable multicast&quot; into google would be a good start - there are some 
</I>&gt;<i> good RFCs produced the the rmt IETF working group.
</I>&gt;<i> 
</I>
Actually I am part of the IRTF group on P2P, E2E and SAM. I know the 
approaches they are being tossed about. I have tried to implement some 
of them. I just am not of the opinion that smart people can't find 
solutions to tough problems.

Is multicast or broadcast the right way? I don't know, but I do know 
that without trying we will never know. Having been part of the IETF 
community for a lot of years (I was part of the group that worked on 
SNMP v1 and the WinSock standard), I know that when the &quot;pedal meets the 
metal&quot; sometimes you discover interesting things.

&gt;&gt;<i>
</I>&gt;&gt;<i> If I use TCP and stick to the serial, synchronized semantics of RPC, 
</I>&gt;&gt;<i> doing one call at a time, I have only a few ways to solve the problem. 
</I>&gt;&gt;<i> Do one call at a time, repeat N times, and that could take quite a 
</I>&gt;&gt;<i> while. I could do M spawnProcesses and have each do N/M RPC calls. Or 
</I>&gt;&gt;<i> I could use M threads and do it that way. Granted I have M sockets 
</I>&gt;&gt;<i> open at a time, it is possible for this to take quite a while to 
</I>&gt;&gt;<i> execute. Performance would be terrible (and yes I want an approach 
</I>&gt;&gt;<i> that has good to very good performance. After all who would want poor 
</I>&gt;&gt;<i> to terrible performance?)
</I>&gt;<i> 
</I>&gt;<i> Knuth and his comments on early optimisation apply here. Have you tried 
</I>&gt;<i> it? You might be surprised.
</I>&gt;<i>
</I>
I am sorry to say I don't know the paper or research you are referring 
to. Can you point me to some references?


&gt;<i> I have some Twisted code that does SNMP to over a thousand devices. This 
</I>&gt;<i> is, obviously, unicast UDP. The throughput is very high. A simple 
</I>&gt;<i> ACK-based sequence-numbered UDP unicast will very likely scale to 
</I>&gt;<i> thousands of nodes.
</I>&gt;<i>
</I>
Thanks for the information. This is what makes me think that I want 
something based on UDP and not TCP! And if I can do RMT (or some variant 
of it) I might be able to get better performance. But, as I said it is 
the nice thing about not having someone telling me I need to get a 
product out the door tomorrow! I have time to experiment and learn.


&gt;&gt;<i>
</I>&gt;&gt;<i> So I divided the problem down to two parts. One, can I reduce the 
</I>&gt;&gt;<i> amount of traffic on the invoking side of the RPC request? Second, is 
</I>&gt;&gt;<i> how to deal with the response. Obviously I have to deal with the issue 
</I>&gt;&gt;<i> of failure, since RPC semantics require EXACTLY-ONCE.
</I>&gt;<i> 
</I>&gt;<i> How many calls per second are you doing, and approximately what volume 
</I>&gt;<i> of data will each call exchange?
</I>&gt;<i> 
</I>This is information I can't provide since the system I have designing 
has no equivalent in the marketplace today (either commercial or open 
source). All I know is that the first version of the system I built - 
using C/C++ and a traditional architecture (a few dozens of machines) 
was able to handle 200 transactions/minute (using SOAP). While there 
were some &quot;short messages&quot; (less than an normal MTU), I had quite a few 
that topped out 50K bytes and some up to 100Mbytes.

Doing some research I have been told to expect a great many short ones 
and many very long ones; sort of an inverted bell curve. But there are 
very few real statistics. As I said I have to put a stake in the ground 
and build something so I am guessing where the problems might rest and 
trying to find some solutions for them. Hence my query.

&gt;<i> You seem inflexible about aspects of the design. If if were me, I'd 
</I>&gt;<i> abandon RPC semantics. Smarter people than anyone here have argued 
</I>&gt;<i> convincingly against making a remote procedure call look anything like a 
</I>&gt;<i> local one, and once you abandon *that*, RPCs look like message exchanges.
</I>&gt;<i>
</I>
I agree. I am not sure where the answer lies. I like Twisted because it 
affords a nice way to experiment with different mechanisms both at the 
transport and the semantic layer. I am looking for ideas! As I said I 
have the time and inclination to experiment. What I need are things that 
aren't obvious (because I haven't heard of them or thought of them).

&gt;&gt;<i>
</I>&gt;&gt;<i> That gets me to the multicast or broadcast scheme. In one call I could 
</I>&gt;&gt;<i> get the N processors to start working. Now I just have to solve the 
</I>&gt;&gt;<i> other half of the problem: how to get the answers returned without 
</I>&gt;&gt;<i> swamping the network or how to detect when I didn't get an answer from 
</I>&gt;&gt;<i> a processor at all.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> That leads me to the observation that on an uncongested ethernet I 
</I>&gt;&gt;<i> almost always have a successful transmission. This means I have to deal 
</I>&gt;<i> 
</I>&gt;<i> Successful transmission is really the easy bit for multicast. There is 
</I>&gt;<i> IGMP snooping, IGMP querier misbehaviour, loss of forwarding on an 
</I>&gt;<i> upstream IGP flap, flooding issues due to global MSDP issues, and so forth.
</I>&gt;<i> 
</I>
I agree about the successful transmission. You've lost me on the IGMP 
part. Can you elaborate as to your thoughts?

&gt;&gt;<i> with that issue and a few others. Why do I care? Because I believe I 
</I>&gt;&gt;<i> can accomplish what I need - get great performance most of the time, 
</I>&gt;&gt;<i> and only in a few instances have to deal with do the operation over 
</I>&gt;&gt;<i> again.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> This is a tough problem to solve. I am not sure of the outcome but I 
</I>&gt;&gt;<i> am sure that I need to start somewhere. What I know is that it is 
</I>&gt;&gt;<i> partly transport and partly marshalling. The semantics of the call 
</I>&gt;&gt;<i> have to stay fixed: EXACTLY-ONCE.
</I>&gt;<i> 
</I>&gt;<i> If you MUST have EXACTLY-ONCE group communication semantics, you should 
</I>&gt;<i> use a message bus.
</I>&gt;<i> 
</I>
I do know I need EXACTLY-ONCE semantics but how and where I implement 
them is the unknown. When you use TCP you assume the network provides 
the bulk of the solution. I have been thinking that if I use a less 
reliable network - one with low overhead - that I can provide the server 
part to do the EXACTLY-ONCE piece.

As to why I need EXACTLY-ONCE, well if I have to store something I know 
I absolutely need to store it. I can't be in the position that I don't 
know it has been stored - it must be there.

Thanks for the great remarks....I look forward to reading more.

Chaz


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="046400.html">[Twisted-Python] Multicast XMLRPC
</A></li>
	<LI>Next message (by thread): <A HREF="046402.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46401">[ date ]</a>
              <a href="thread.html#46401">[ thread ]</a>
              <a href="subject.html#46401">[ subject ]</a>
              <a href="author.html#46401">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
