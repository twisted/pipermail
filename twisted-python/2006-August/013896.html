<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Multicast XMLRPC
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=44F05249.2060803%40imperial.ac.uk">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="013895.html">
   <LINK REL="Next"  HREF="013897.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Multicast XMLRPC</H1>
    <B>Chaz.</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=44F05249.2060803%40imperial.ac.uk"
       TITLE="[Twisted-Python] Multicast XMLRPC">eprparadocs at gmail.com
       </A><BR>
    <I>Sat Aug 26 10:14:48 EDT 2006</I>
    <P><UL>
        <LI>Previous message: <A HREF="013895.html">[Twisted-Python] Multicast XMLRPC
</A></li>
        <LI>Next message: <A HREF="013897.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#13896">[ date ]</a>
              <a href="thread.html#13896">[ thread ]</a>
              <a href="subject.html#13896">[ subject ]</a>
              <a href="author.html#13896">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Phil Mayers wrote:
&gt;<i> Chaz. wrote:
</I>&gt;<i> 
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I started out using Spread some time ago (more than 2 years ago). The 
</I>&gt;&gt;<i> implementation was limited to a hundred or so nodes (that is in the 
</I>&gt;&gt;<i> notes on the spread implementation). Secondly it isn't quite so 
</I>&gt;&gt;<i> lightweight as you think (I've measured the performance).
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> It is a very nice system but when it gets to 1000s of machines very 
</I>&gt;&gt;<i> little work has been done on solving many of the problems. My research 
</I>&gt;&gt;<i> on it goes back almost a decade starting out with Horus.
</I>&gt;<i> 
</I>&gt;<i> I must admit to not having attempted to scale it that far, but I was 
</I>&gt;<i> under the impression that only the more expensive delivery modes were 
</I>&gt;<i> that costly. But by the sounds of it, you don't need me to tell you that.
</I>&gt;<i> 
</I>
Originally I had started out thinking about work surrounding Horus and 
researched a lot of the group communication stuff. When I got to Spread 
I tried it thinking it would solve all my problems. I actually built a 
system using it only to be sadly disappointed.

First I hit the 100+ node limit. Then I got to the static configuration, 
which I spent time trying to overcome. Finally when I did some 
measurements I decided that 1000s of machines would require a 
&quot;hub-and-spoke&quot; like architecture that Glyph suggested. I decided it was 
much too complicated and backed away.

Since that time I let the pendulum swing to the other extreme - no 
predefined architecture (or aggregation of machines). I want to examine 
what happens when I have 1000s of machines without a topology; can I 
solve the problems. As I said I solved the decentralized membership list 
issue. Now I am on to the harder problem: can I get RPC-like semantics 
with reasonable performance over the 1000s of machines? I don't know.

&gt;&gt;<i>
</I>&gt;&gt;<i> Actually I am part of the IRTF group on P2P, E2E and SAM. I know the 
</I>&gt;&gt;<i> approaches they are being tossed about. I have tried to implement some 
</I>&gt;&gt;<i> of them. I just am not of the opinion that smart people can't find 
</I>&gt;&gt;<i> solutions to tough problems.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> Ok, in which case my apologies. My reading of your posts had lead me to 
</I>&gt;<i> believe, incorrectly, you may not be familiar with the various issues. 
</I>&gt;<i> In that case, you can (should) disregard most of it.
</I>&gt;<i> 
</I>
I think the problem is on my part. I asked what I thought was an obvious 
question without laying the groundwork as to what I knew or how.

&gt;&gt;<i>
</I>&gt;&gt;<i> Is multicast or broadcast the right way? I don't know, but I do know 
</I>&gt;&gt;<i> that without trying we will never know. Having been part of the IETF 
</I>&gt;<i> 
</I>&gt;<i> It's clearly right for some things - I'm just not sure how much 
</I>&gt;<i> bi-directional distribution would be helped by it, since you've got at 
</I>&gt;<i> some point to get the replies back.
</I>&gt;<i>
</I>
I think I feel comfortable with using multicast (or broadcast) for the 
invoking RPC call. What I don't have a clear feeling for is how to 
correctly handle the response - I know I can't send them all within some 
small delta without congesting the network. So I am looking at all sorts 
of techniques (like holding off the responses, randomly...but I don't 
know how that will impact retries, etc).

&gt;&gt;<i> community for a lot of years (I was part of the group that worked on 
</I>&gt;&gt;<i> SNMP v1 and the WinSock standard), I know that when the &quot;pedal meets 
</I>&gt;&gt;<i> the metal&quot; sometimes you discover interesting things.
</I>&gt;<i> 
</I>&gt;<i> I didn't realise winsock went near the IETF. You learn something new 
</I>&gt;<i> every day.
</I>&gt;<i> 
</I>
Me too!

&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Knuth and his comments on early optimisation apply here. Have you 
</I>&gt;&gt;&gt;<i> tried it? You might be surprised.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I am sorry to say I don't know the paper or research you are referring 
</I>&gt;&gt;<i> to. Can you point me to some references?
</I>&gt;<i> 
</I>&gt;<i> Sorry, it's a phrase from Donald Knuth's (excellent) three-volume 
</I>&gt;<i> programming book, &quot;The Art of Computer Programming&quot;. Highly recommended.
</I>&gt;<i>
</I>
Ah, ok. Having read them so many years ago I forgot most of it. lol..


&gt;&gt;<i>
</I>&gt;&gt;<i> Thanks for the information. This is what makes me think that I want 
</I>&gt;&gt;<i> something based on UDP and not TCP! And if I can do RMT (or some 
</I>&gt;&gt;<i> variant of it) I might be able to get better performance. But, as I 
</I>&gt;&gt;<i> said it is the nice thing about not having someone telling me I need 
</I>&gt;&gt;<i> to get a product out the door tomorrow! I have time to experiment and 
</I>&gt;&gt;<i> learn.
</I>&gt;<i> 
</I>&gt;<i> When I wrote my reply I hadn't seen your comment on the app being 
</I>&gt;<i> distributed storage.
</I>&gt;<i> 
</I>&gt;&gt;&gt;<i> How many calls per second are you doing, and approximately what 
</I>&gt;&gt;&gt;<i> volume of data will each call exchange?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i> This is information I can't provide since the system I have designing 
</I>&gt;&gt;<i> has no equivalent in the marketplace today (either commercial or open 
</I>&gt;&gt;<i> source). All I know is that the first version of the system I built - 
</I>&gt;&gt;<i> using C/C++ and a traditional architecture (a few dozens of machines) 
</I>&gt;&gt;<i> was able to handle 200 transactions/minute (using SOAP). While there 
</I>&gt;&gt;<i> were some &quot;short messages&quot; (less than an normal MTU), I had quite a 
</I>&gt;&gt;<i> few that topped out 50K bytes and some up to 100Mbytes.
</I>&gt;<i> 
</I>&gt;<i> Oh. In which case more or less everything I wrote is useless!
</I>&gt;<i> 
</I>
Well I don't think so. Based on your multicast comment I wonder about 
broadcast...have you ever seen the same thing happen? When you say 
&quot;short interruptions&quot; are we talking more than seconds? Can you 
elaborate a bit?

&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Successful transmission is really the easy bit for multicast. There 
</I>&gt;&gt;&gt;<i> is IGMP snooping, IGMP querier misbehaviour, loss of forwarding on an 
</I>&gt;&gt;&gt;<i> upstream IGP flap, flooding issues due to global MSDP issues, and so 
</I>&gt;&gt;&gt;<i> forth.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I agree about the successful transmission. You've lost me on the IGMP 
</I>&gt;&gt;<i> part. Can you elaborate as to your thoughts?
</I>&gt;<i> 
</I>&gt;<i> Well, my experience of large multicast IPv4 networks is that short 
</I>&gt;<i> interruptions in multicast connectivity are not uncommon. There are a 
</I>&gt;<i> number of reasons for this, which can be broadly broken down into 1st 
</I>&gt;<i> hop and subsequent hop issues.
</I>&gt;<i> 
</I>&gt;<i> Basically, in a routed-multicast environment, I've seen the subnet IGMP 
</I>&gt;<i> querier (normally the gateway) get pre-empted by badly configured or 
</I>&gt;<i> plain broken OS stacks (e.g. someone running Linux with the IGMPv3 early 
</I>&gt;<i> patches). I've also seen confusion for highly-available subnets (e.g. 
</I>&gt;<i> VRRPed networks) where the IGMP querier and the multicast designated 
</I>&gt;<i> forwarder are different. This can cause issues with the IGMP snooping on 
</I>&gt;<i> the downstream layer2 switches when the DF is no longer on the path 
</I>&gt;<i> which the layer2 snooping builds.
</I>&gt;<i> 
</I>&gt;<i> You also get issues with upstream changes in the unicast routing 
</I>&gt;<i> topology affecting PIM.
</I>&gt;<i> 
</I>&gt;<i> Most of these are only issues with routed multicast. Subnet-local is a 
</I>&gt;<i> lot simpler, though you do still need an IGMP querier and switches with 
</I>&gt;<i> IGMP snooping.
</I>&gt;<i> 
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I do know I need EXACTLY-ONCE semantics but how and where I implement 
</I>&gt;&gt;<i> them is the unknown. When you use TCP you assume the network provides 
</I>&gt;&gt;<i> the bulk of the solution. I have been thinking that if I use a less 
</I>&gt;&gt;<i> reliable network - one with low overhead - that I can provide the 
</I>&gt;&gt;<i> server part to do the EXACTLY-ONCE piece.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> As to why I need EXACTLY-ONCE, well if I have to store something I 
</I>&gt;&gt;<i> know I absolutely need to store it. I can't be in the position that I 
</I>&gt;&gt;<i> don't know it has been stored - it must be there.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Thanks for the great remarks....I look forward to reading more.
</I>&gt;<i> 
</I>&gt;<i> This makes a lot more sense now I know it's storage related.
</I>&gt;<i> 
</I>&gt;<i> You're right, this is a tricky and uncommon problem.
</I>&gt;<i> 
</I>&gt;<i> Let me see if I've got this right:
</I>&gt;<i> 
</I>&gt;<i> You're building some kind of distributed storage service. Clients will 
</I>&gt;<i> access the storage by a &quot;normal&quot; protocol to one of the nodes. Reads 
</I>&gt;<i> from the store are relatively easy, but writes to the store will need to 
</I>&gt;<i> be distributed to all or a subset of the nodes. Obviously you'll have a 
</I>&gt;<i> mix of lots of small writes and some very large writes.
</I>&gt;<i> 
</I>&gt;<i> Hmm.
</I>&gt;<i> 
</I>&gt;<i> Are you envisioning that you might have &gt;1 storage set on the nodes, and 
</I>&gt;<i> using a different multicast group per storage set to build optimal 
</I>&gt;<i> distribution?
</I>&gt;<i> 
</I>&gt;<i> You might be able to perform some tricks depending on whether this 
</I>&gt;<i> service provides block- or filesystem-level semantics. If it's the 
</I>&gt;<i> latter, you could import some techniques from the distributed version 
</I>&gt;<i> control arena - broadly speaking, node+version number each file and 
</I>&gt;<i> &quot;broadcast&quot; (in the application sense) just the file + 
</I>&gt;<i> newnode+newversion to the other store nodes, and have them lock the 
</I>&gt;<i> local copy and initiate a pull from the updated node.
</I>&gt;<i> 
</I>&gt;<i> For block-level storage, that's going to be a lot harder.
</I>&gt;<i> 
</I>Definitely! Right now I dealing on the filesystem level. Doing block 
level would be incredibly difficult. I am trying to solve the simpler 
problem first! lol.

&gt;<i> For the multicast, something like NORM, which as you probably know is 
</I>&gt;<i> basically forward-error-corrected transmit channel with 
</I>&gt;<i> receiver-triggered re-transmits, would probably work. An implementation 
</I>&gt;<i> would likely be non-trivial, but a fascinating project.
</I>&gt;<i> 
</I>

Right now I am trying to find a solution to an interesting problem: how 
to find a file without knowing exactly where it exists in the network. 
You have to do this to make the system scale nicely.

Basically each node holds information about the files (aka objects) it 
stores. I do this so that I don't have a central database any where 
(this allows the system to scale differently. With a central database I 
would have that set of servers scale differently than the storage nodes).

Now I can build a set of machines that are the distributed database 
machines - each storing something - and querying them for where the file 
lives; this would narrow the machines I have to directly talk to, but it 
feels wrong. This is sort of a variation of the hub-and-spoke that Glyph 
talked about. But having said that I am trying to determine if I can get 
away from that and just go to a very unstructured environment (without 
intermediate database nodes).

As I said I have time to experiment before I put the code in the open 
source community ...

Peace,
Chaz


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="013895.html">[Twisted-Python] Multicast XMLRPC
</A></li>
	<LI>Next message: <A HREF="013897.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#13896">[ date ]</a>
              <a href="thread.html#13896">[ thread ]</a>
              <a href="subject.html#13896">[ subject ]</a>
              <a href="author.html#13896">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
