<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Multicast XMLRPC
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44EF44DF.8030401%40gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="046388.html">
   <LINK REL="Next"  HREF="046393.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Multicast XMLRPC</H1>
    <B>Chaz.</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44EF44DF.8030401%40gmail.com%3E"
       TITLE="[Twisted-Python] Multicast XMLRPC">eprparadocs at gmail.com
       </A><BR>
    <I>Fri Aug 25 12:43:43 MDT 2006</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="046388.html">[Twisted-Python] Multicast XMLRPC
</A></li>
        <LI>Next message (by thread): <A HREF="046393.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46390">[ date ]</a>
              <a href="thread.html#46390">[ thread ]</a>
              <a href="subject.html#46390">[ subject ]</a>
              <a href="author.html#46390">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE><A HREF="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">glyph at divmod.com</A> wrote:
&gt;<i> On Fri, 25 Aug 2006 13:07:49 -0400, &quot;Chaz.&quot; &lt;<A HREF="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">eprparadocs at gmail.com</A>&gt; wrote:
</I>&gt;<i> 
</I>&gt;&gt;<i> I will state what I thought was obvious: I need to make &quot;calls&quot; to 
</I>&gt;&gt;<i> thousands of machines to do something. I want to minimize the overhead 
</I>&gt;&gt;<i> both of making the call and the machines sending back the responses.
</I>&gt;<i> 
</I>&gt;<i> Maybe you could start off a little bit further back into the problem 
</I>&gt;<i> today.  Like, &quot;I got up this morning, and I thought, 'I would like some 
</I>&gt;<i> toast.', but I don't know how to make toast, so I wanted to design a 
</I>&gt;<i> 100,000 node parallel neural network to develop a receipie for toast.&quot;
</I>&gt;<i> 
</I>&gt;<i> Perhaps then someone on this list could relate their toast development 
</I>&gt;<i> experiences, such as &quot;using a TCP-based tree topology similar to IRC 
</I>&gt;<i> servers has been sufficient in my experience for toast-oriented data 
</I>&gt;<i> exchange although I have been using a parallelized coordinated genetic 
</I>&gt;<i> algorithm rather than a neural network to develop an optimal 
</I>&gt;<i> crunch/warmth experience&quot;, or possibly &quot;ToastVortex, my Twisted-basted 
</I>&gt;<i> toast application server is available at 
</I>&gt;<i> <A HREF="http://toastvortex.example.com/">http://toastvortex.example.com/</A>&quot; or better yet, &quot;buy a toaster and put 
</I>&gt;<i> some bread in it&quot;.
</I>&gt;<i> 
</I>&gt;&gt;<i> TCP is pretty resource intensive so I need something else. I think a 
</I>&gt;&gt;<i> reliable datagram service on top of some underlying transport is the 
</I>&gt;&gt;<i> way to go (on top of multicast/broadcast/IP is what I am thinking about).
</I>&gt;<i> 
</I>&gt;<i> TCP's &quot;resource&quot; consumption is localized in an a highly optimized 
</I>&gt;<i> environment; in OS kernels, where the TCP stack is tuned constantly by 
</I>&gt;<i> thousands of people, in routing hardware that is specialized to give TCP 
</I>&gt;<i> traffic priority to improve performance, and in the guts of the public 
</I>&gt;<i> internet that runs such hardware and is constantly monitored and tweaked 
</I>&gt;<i> to give TCP even more of a boost.  Any custom multicast protocol you 
</I>&gt;<i> develop, while perhaps theoretically better than TCP, is possibly going 
</I>&gt;<i> to get swamped by the marginalia that TCP has spent decades 
</I>&gt;<i> eradicating.  In Python, you're going to be doing a lot of additional 
</I>&gt;<i> CPU work.  For example, TCP acks often won't even be promoted to 
</I>&gt;<i> userspace, whereas you're going to need to process every unicast 
</I>&gt;<i> acknowledgement to your multicast message separately in userspace.
</I>&gt;<i> 
</I>&gt;<i> While my toast network deployments are minimal, I *have* written quite a 
</I>&gt;<i> few multi-unicast servers, some of which processed quite a high volume 
</I>&gt;<i> of traffic acceptably, and in at least one case this work was later 
</I>&gt;<i> optimized by another developer who spent months working on a multicast 
</I>&gt;<i> replacement.  That replacement which was later abandoned because the 
</I>&gt;<i> deployment burden of a large-scale multicast-capable network was huge.  
</I>&gt;<i> That's to say nothing of the months of additional time required to 
</I>&gt;<i> develop and properly *test* such a beast.
</I>&gt;<i> 
</I>&gt;<i> You haven't said what resources TCP is consuming which are unacceptble, 
</I>&gt;<i> however,  Is it taking too much system time?  Too much local bandwidth?  
</I>&gt;<i> Is your ethernet experiencing too many collisions?  Are you concerned 
</I>&gt;<i> about the cost of public internet bandwidth overages with your service 
</I>&gt;<i> provider?  What's your network topology?  It would be hard to list the 
</I>&gt;<i> answers to all of these questions (or even exhaustively ask all the 
</I>&gt;<i> questions one would need to comment usefully) but one might at least 
</I>&gt;<i> make guesses that did not fall too wide of the mark if one knew what the 
</I>&gt;<i> application in question were actually *doing*.
</I>&gt;<i> 
</I>&gt;<i> In any event, XML-RPC is hardly a protocol which is famous for its low 
</I>&gt;<i> resource consumption on any of these axes, so if you're driven by 
</I>&gt;<i> efficiency concerns, it seems an odd choice to layer on top of a 
</I>&gt;<i> hand-tuned multicast-request/unicast-response protocol.
</I>&gt;<i> 
</I>&gt;<i> _______________________________________________
</I>&gt;<i> Twisted-Python mailing list
</I>&gt;<i> <A HREF="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">Twisted-Python at twistedmatrix.com</A>
</I>&gt;<i> <A HREF="http://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">http://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python</A>
</I>&gt;<i> 
</I>
Perhaps the simple way to say this is that I need to do group 
communications that support RPC semantics with minimal overhead.

You ask about the network topology; all I can say is that it supports 
the normal communication means: unicast, broadcast and maybe multicast. 
I am being intentionally vague since I don't want to have any specific 
network architecture.

I don't want to use overlay networks, if at all possible. While they are 
nice, I would prefer something a little more direct (though that might 
not be possible). The reason? Direct operations are faster.

I have a membership list of the state of all the processors in the 
system (and I am talking 1000's of processors) without the use of 
standard heartbeat (in the traditional use of heartbeat I would have N! 
ping messages!). I figured out probabilistic polling with gossip was enough.

I don't particular care if it is PB, XML-RPC or SOAP as the marshalling 
mechanism. I mention them since they allow me to solve one problem at a 
time. I would like to build the solution a piece at a time to do some 
measurements and testing. Today the underlying transport and tomorrow 
the marshallings.

Now let me address the issue of TCP. It is a pretty heavy protocol to 
use. It takes a lot of resources on the sender and target and can take 
some time to establish a connection. Opening a 1000 or more sockets 
consumes a lot of resources in the underlying OS and in the Twisted client!

If I use TCP and stick to the serial, synchronized semantics of RPC, 
doing one call at a time, I have only a few ways to solve the problem. 
Do one call at a time, repeat N times, and that could take quite a 
while. I could do M spawnProcesses and have each do N/M RPC calls. Or I 
could use M threads and do it that way. Granted I have M sockets open at 
a time, it is possible for this to take quite a while to execute. 
Performance would be terrible (and yes I want an approach that has good 
to very good performance. After all who would want poor to terrible 
performance?)

So I divided the problem down to two parts. One, can I reduce the amount 
of traffic on the invoking side of the RPC request? Second, is how to 
deal with the response. Obviously I have to deal with the issue of 
failure, since RPC semantics require EXACTLY-ONCE.

That gets me to the multicast or broadcast scheme. In one call I could 
get the N processors to start working. Now I just have to solve the 
other half of the problem: how to get the answers returned without 
swamping the network or how to detect when I didn't get an answer from a 
processor at all.

That leads me to the observation that on an uncongested ethernet I 
almost always have a successful transmission. This means I have to deal 
with that issue and a few others. Why do I care? Because I believe I can 
accomplish what I need - get great performance most of the time, and 
only in a few instances have to deal with do the operation over again.

This is a tough problem to solve. I am not sure of the outcome but I am 
sure that I need to start somewhere. What I know is that it is partly 
transport and partly marshalling. The semantics of the call have to stay 
fixed: EXACTLY-ONCE.

Hope this helps cast the problem...I didn't mean to sound terse before I 
just figured everyone had already thought about the problem and knew the 
issues.

Chaz



</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="046388.html">[Twisted-Python] Multicast XMLRPC
</A></li>
	<LI>Next message (by thread): <A HREF="046393.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46390">[ date ]</a>
              <a href="thread.html#46390">[ thread ]</a>
              <a href="subject.html#46390">[ subject ]</a>
              <a href="author.html#46390">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
