<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Multicast XMLRPC
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44F032E8.6070103%40imperial.ac.uk%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="046399.html">
   <LINK REL="Next"  HREF="046401.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Multicast XMLRPC</H1>
    <B>Phil Mayers</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Multicast%20XMLRPC&In-Reply-To=%3C44F032E8.6070103%40imperial.ac.uk%3E"
       TITLE="[Twisted-Python] Multicast XMLRPC">p.mayers at imperial.ac.uk
       </A><BR>
    <I>Sat Aug 26 05:39:20 MDT 2006</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="046399.html">[Twisted-Python] Multicast XMLRPC
</A></li>
        <LI>Next message (by thread): <A HREF="046401.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46400">[ date ]</a>
              <a href="thread.html#46400">[ thread ]</a>
              <a href="subject.html#46400">[ subject ]</a>
              <a href="author.html#46400">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Chaz. wrote:
&gt;<i> 
</I>&gt;<i> Now let me address the issue of TCP. It is a pretty heavy protocol to 
</I>&gt;<i> use. It takes a lot of resources on the sender and target and can take 
</I>&gt;<i> some time to establish a connection. Opening a 1000 or more sockets 
</I>&gt;<i> consumes a lot of resources in the underlying OS and in the Twisted client!
</I>
People keep trying to help you, and you keep repeating yourself. From 
what I can gather:

You *need* a relatively lightweight group communication method. My 
advice would be to investigate a message bus - see recent posts on this 
mailing list. &quot;Spread&quot; at www.spread.org and ActiveMQ (via the simple 
text-over-tcp-based STOMP protocol). Reports are that both can (under 
the right conditions) execute many thousands of group messages per second.

Failing that, Glyph has hinted at another approach. You could elect a 
small number (~1%) of your nodes as &quot;proxies&quot; so that as well as being 
clients, they act as intermediaries for messages. This is a simple form 
of overlay network, which you also stated you didn't want to use - lord 
knows why. People use these techniques for a reason - they work.

You *want* (have decided you want) a reliable multicast protocol over 
which you'll layer a simple RPC protocol. RMT (reliable multicast 
transport) is as yet an unsolved problem. It is VERY VERY hard. None 
exist for Twisted, to the best of my knowledge. I would be willing to 
bet money that, for &quot;thousands&quot; of nodes, the overhead of implementing 
such a protocol (in Python, one presumes) would exceed the overhead of 
just using TCP. If you had said &quot;hundreds of thousands&quot; of nodes, well, 
that would be different.

If you want to knock an RMT up based on the assumption you won't drop 
packets, then be my guest, but I would suggest that if you *really* 
believe multicast is that reliable, then your experience of IP multicast 
networks has been a lot more rosy than mine, and I run a very large one.

&quot;reliable multicast&quot; into google would be a good start - there are some 
good RFCs produced the the rmt IETF working group.

&gt;<i> 
</I>&gt;<i> If I use TCP and stick to the serial, synchronized semantics of RPC, 
</I>&gt;<i> doing one call at a time, I have only a few ways to solve the problem. 
</I>&gt;<i> Do one call at a time, repeat N times, and that could take quite a 
</I>&gt;<i> while. I could do M spawnProcesses and have each do N/M RPC calls. Or I 
</I>&gt;<i> could use M threads and do it that way. Granted I have M sockets open at 
</I>&gt;<i> a time, it is possible for this to take quite a while to execute. 
</I>&gt;<i> Performance would be terrible (and yes I want an approach that has good 
</I>&gt;<i> to very good performance. After all who would want poor to terrible 
</I>&gt;<i> performance?)
</I>
Knuth and his comments on early optimisation apply here. Have you tried 
it? You might be surprised.

I have some Twisted code that does SNMP to over a thousand devices. This 
is, obviously, unicast UDP. The throughput is very high. A simple 
ACK-based sequence-numbered UDP unicast will very likely scale to 
thousands of nodes.

&gt;<i> 
</I>&gt;<i> So I divided the problem down to two parts. One, can I reduce the amount 
</I>&gt;<i> of traffic on the invoking side of the RPC request? Second, is how to 
</I>&gt;<i> deal with the response. Obviously I have to deal with the issue of 
</I>&gt;<i> failure, since RPC semantics require EXACTLY-ONCE.
</I>
How many calls per second are you doing, and approximately what volume 
of data will each call exchange?

You seem inflexible about aspects of the design. If if were me, I'd 
abandon RPC semantics. Smarter people than anyone here have argued 
convincingly against making a remote procedure call look anything like a 
local one, and once you abandon *that*, RPCs look like message exchanges.

&gt;<i> 
</I>&gt;<i> That gets me to the multicast or broadcast scheme. In one call I could 
</I>&gt;<i> get the N processors to start working. Now I just have to solve the 
</I>&gt;<i> other half of the problem: how to get the answers returned without 
</I>&gt;<i> swamping the network or how to detect when I didn't get an answer from a 
</I>&gt;<i> processor at all.
</I>&gt;<i> 
</I>&gt;<i> That leads me to the observation that on an uncongested ethernet I 
</I>&gt;<i> almost always have a successful transmission. This means I have to deal 
</I>
Successful transmission is really the easy bit for multicast. There is 
IGMP snooping, IGMP querier misbehaviour, loss of forwarding on an 
upstream IGP flap, flooding issues due to global MSDP issues, and so forth.

&gt;<i> with that issue and a few others. Why do I care? Because I believe I can 
</I>&gt;<i> accomplish what I need - get great performance most of the time, and 
</I>&gt;<i> only in a few instances have to deal with do the operation over again.
</I>&gt;<i> 
</I>&gt;<i> This is a tough problem to solve. I am not sure of the outcome but I am 
</I>&gt;<i> sure that I need to start somewhere. What I know is that it is partly 
</I>&gt;<i> transport and partly marshalling. The semantics of the call have to stay 
</I>&gt;<i> fixed: EXACTLY-ONCE.
</I>
If you MUST have EXACTLY-ONCE group communication semantics, you should 
use a message bus.


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="046399.html">[Twisted-Python] Multicast XMLRPC
</A></li>
	<LI>Next message (by thread): <A HREF="046401.html">[Twisted-Python] Multicast XMLRPC
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#46400">[ date ]</a>
              <a href="thread.html#46400">[ thread ]</a>
              <a href="subject.html#46400">[ subject ]</a>
              <a href="author.html#46400">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
