<tt>
&lt;br&gt;&lt;br&gt;&lt;div&gt;&lt;span&nbsp;class=&quot;gmail_quote&quot;&gt;On&nbsp;1/17/07,&nbsp;&lt;b&nbsp;class=&quot;gmail_sendername&quot;&gt;&lt;a&nbsp;href=&quot;mailto:glyph@divmod.com&quot;&gt;glyph@divmod.com&lt;/a&gt;&lt;/b&gt;&nbsp;&amp;lt;&lt;a&nbsp;href=&quot;mailto:glyph@divmod.com&quot;&gt;glyph@divmod.com&lt;/a&gt;&amp;gt;&nbsp;wrote:&lt;/span&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;border-left:&nbsp;1px&nbsp;solid&nbsp;rgb(204,&nbsp;204,&nbsp;204);&nbsp;margin:&nbsp;0pt&nbsp;0pt&nbsp;0pt&nbsp;0.8ex;&nbsp;padding-left:&nbsp;1ex;&quot;&gt;<br>
&lt;div&gt;&lt;span&nbsp;class=&quot;q&quot;&gt;On&nbsp;11:35&nbsp;pm,&nbsp;&lt;a&nbsp;href=&quot;mailto:jarrod@vertigrated.com&quot;&nbsp;target=&quot;_blank&quot;&nbsp;onclick=&quot;return&nbsp;top.js.OpenExtLink(window,event,this)&quot;&gt;jarrod@vertigrated.com&lt;/a&gt;&nbsp;wrote:&lt;br&gt;&lt;br&gt;&amp;gt;There&nbsp;is&nbsp;a&nbsp;&amp;quot;backend&amp;quot;&nbsp;C&nbsp;module&nbsp;that&nbsp;our&nbsp;Twisted&nbsp;server&nbsp;front&nbsp;ends,&nbsp;and&nbsp;it&nbsp;is<br>
&lt;br&gt;&amp;gt;highly&nbsp;multi-threaded.&lt;br&gt;&amp;gt;So&nbsp;the&nbsp;T1000&nbsp;is&nbsp;PERFECT&nbsp;for&nbsp;our&nbsp;application,&nbsp;except&nbsp;that&nbsp;now&nbsp;Twisted&nbsp;is&nbsp;the&lt;br&gt;&amp;gt;bottleneck.&nbsp;:-(&lt;br&gt;&lt;br&gt;&lt;/span&gt;This&nbsp;seems&nbsp;odd&nbsp;to&nbsp;me.&lt;br&gt;&lt;br&gt;If&nbsp;all&nbsp;the&nbsp;CPUs&nbsp;are&nbsp;going&nbsp;to&nbsp;be&nbsp;busy&nbsp;doing&nbsp;a&nbsp;multi-threaded&nbsp;back-end&amp;#39;s&nbsp;work,&nbsp;and&nbsp;Twisted&nbsp;is&nbsp;just&nbsp;doing&nbsp;the&nbsp;I/O,&nbsp;then&nbsp;it&nbsp;seems&nbsp;the&nbsp;T1000&nbsp;would&nbsp;still&nbsp;be&nbsp;a&nbsp;benefit.&nbsp;&amp;nbsp;The&nbsp;benchmark&nbsp;you&nbsp;mentioned&nbsp;was&nbsp;completely&nbsp;static;&nbsp;there&nbsp;was&nbsp;no&nbsp;backend&nbsp;library,&nbsp;no&nbsp;multithreaded&nbsp;CPU&nbsp;load.&nbsp;&amp;nbsp;Is&nbsp;the&nbsp;performance&nbsp;disparity&nbsp;similar&nbsp;when&nbsp;you&amp;#39;re&nbsp;running&nbsp;actual&nbsp;workloads?<br>
&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;snipped&nbsp;a&amp;nbsp;&nbsp;lot&nbsp;of&nbsp;good&nbsp;information&nbsp;:-)&nbsp;&lt;br&gt;&lt;/div&gt;&lt;br&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;border-left:&nbsp;1px&nbsp;solid&nbsp;rgb(204,&nbsp;204,&nbsp;204);&nbsp;margin:&nbsp;0pt&nbsp;0pt&nbsp;0pt&nbsp;0.8ex;&nbsp;padding-left:&nbsp;1ex;&quot;&gt;&lt;div&gt;Again,&nbsp;it&nbsp;seems&nbsp;weird&nbsp;to&nbsp;me&nbsp;that&nbsp;this&nbsp;is&nbsp;necessary&nbsp;if&nbsp;the&nbsp;back-end&nbsp;library&nbsp;is&nbsp;really&nbsp;utilizing&nbsp;all&nbsp;the&nbsp;CPUs&nbsp;already&nbsp;and&nbsp;you&nbsp;are&nbsp;not&nbsp;I/O&nbsp;bound.<br>
&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;Here&nbsp;is&nbsp;what&nbsp;we&nbsp;are&nbsp;doing&nbsp;basically.&lt;br&gt;&lt;br&gt;Twisted&nbsp;takes&nbsp;in&nbsp;data&nbsp;and&nbsp;in&nbsp;a&nbsp;C&nbsp;extension&nbsp;we&nbsp;send&nbsp;the&nbsp;data&nbsp;to&nbsp;multiple&nbsp;backends&nbsp;in&nbsp;parallel&nbsp;to&nbsp;do&nbsp;processing&nbsp;on&nbsp;it.&lt;br&gt;Then&nbsp;we&nbsp;aggregate&nbsp;the&nbsp;results&nbsp;and&nbsp;send&nbsp;information&nbsp;back&nbsp;to&nbsp;the&nbsp;client.<br>
&lt;br&gt;This&nbsp;is&nbsp;basically&nbsp;a&nbsp;fancy&nbsp;proxy&nbsp;that&nbsp;parallelizes&nbsp;and&nbsp;distributes&nbsp;work&nbsp;to&nbsp;other&nbsp;machines&nbsp;on&nbsp;the&nbsp;network.&lt;br&gt;All&nbsp;the&nbsp;clients&nbsp;run&nbsp;in&nbsp;&amp;quot;keep-alive&amp;quot;&nbsp;mode,&nbsp;so&nbsp;they&nbsp;don&amp;#39;t&nbsp;create&nbsp;new&nbsp;connections&nbsp;for&nbsp;each&nbsp;piece&nbsp;of&nbsp;work&nbsp;they&nbsp;send&nbsp;to&nbsp;the&nbsp;system,&nbsp;so<br>
&lt;br&gt;once&nbsp;they&nbsp;are&nbsp;all&nbsp;connected,&nbsp;they&nbsp;stay&nbsp;connected&nbsp;for&nbsp;their&nbsp;lifetime&nbsp;(&nbsp;long&nbsp;time&nbsp;).&lt;br&gt;&lt;br&gt;On&nbsp;the&nbsp;Dell&nbsp;2850&amp;#39;s&nbsp;without&nbsp;any&nbsp;backend&nbsp;code,&nbsp;we&nbsp;see&nbsp;600ms&nbsp;latency&nbsp;with&nbsp;a&nbsp;test&nbsp;suite&nbsp;of&nbsp;400&nbsp;clients.&lt;br&gt;With&nbsp;the&nbsp;Solaris&nbsp;SPARC&nbsp;machines&nbsp;T1000&nbsp;and&nbsp;V210&nbsp;we&nbsp;see&nbsp;4000&nbsp;-&nbsp;5000&nbsp;ms&nbsp;latency&nbsp;with&nbsp;the&nbsp;same&nbsp;no-op&nbsp;code&nbsp;and&nbsp;the&nbsp;same&nbsp;400&nbsp;clients.<br>
&lt;br&gt;&lt;br&gt;With&nbsp;the&nbsp;backend&nbsp;code&nbsp;we&nbsp;see&nbsp;about&nbsp;an&nbsp;additional&nbsp;250ms&nbsp;latency&nbsp;on&nbsp;both&nbsp;platforms,&nbsp;since&nbsp;the&nbsp;&amp;quot;backend&amp;quot;&nbsp;code&nbsp;is&nbsp;just&nbsp;taking&nbsp;the&nbsp;data&nbsp;and&nbsp;sending&nbsp;it&nbsp;out&nbsp;across&nbsp;the&nbsp;network&nbsp;to&nbsp;process,&nbsp;it&nbsp;just&nbsp;sits&nbsp;waiting&nbsp;on&nbsp;responses.&nbsp;The&nbsp;backend&nbsp;code&nbsp;is&nbsp;just&nbsp;not&nbsp;doing&nbsp;enough&nbsp;work&nbsp;to&nbsp;stress&nbsp;the&nbsp;machine&nbsp;basically.<br>
&lt;br&gt;&lt;br&gt;We&nbsp;have&nbsp;LOTS&nbsp;and&nbsp;LOTS&nbsp;of&nbsp;test&nbsp;harness&nbsp;code&nbsp;and&nbsp;profiling&nbsp;code&nbsp;to&nbsp;pinpoint&nbsp;where&nbsp;bottlenecks&nbsp;are.&nbsp;We&nbsp;are&nbsp;going&nbsp;to&nbsp;have&nbsp;process&nbsp;a&nbsp;couple&nbsp;of&nbsp;terabytes&nbsp;a&nbsp;day&nbsp;thru&nbsp;this&nbsp;system.&nbsp;Latency&nbsp;thru&nbsp;the&nbsp;system&nbsp;is&nbsp;a&nbsp;high&nbsp;priority&nbsp;because&nbsp;of&nbsp;what&nbsp;kind&nbsp;of&nbsp;system&nbsp;it&nbsp;is.<br>
&lt;br&gt;&lt;br&gt;We&nbsp;can&nbsp;get&nbsp;up&nbsp;to&nbsp;about&nbsp;1400&nbsp;clients&nbsp;on&nbsp;the&nbsp;Dell&nbsp;2850&nbsp;hardware&nbsp;before&nbsp;latency&nbsp;starts&nbsp;climbing&nbsp;out&nbsp;of&nbsp;control.&lt;br&gt;The&nbsp;SPARC&nbsp;hardware&nbsp;is&nbsp;falling&nbsp;over&nbsp;at&nbsp;400&nbsp;clients&nbsp;:-(&lt;br&gt;&lt;br&gt;Thanks&nbsp;to&nbsp;everyone&nbsp;for&nbsp;all&nbsp;the&nbsp;ideas&nbsp;and&nbsp;help.<br>
&lt;br&gt;&lt;/div&gt;&lt;/div&gt;<br>

</tt>
