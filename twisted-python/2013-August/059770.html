<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Operating on metainformation in a distributed system
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Operating%20on%20metainformation%20in%20a%20distributed%0A%20system&In-Reply-To=%3C520447FA.2070909%40thieprojects.ch%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="027306.html">
   <LINK REL="Next"  HREF="059771.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Operating on metainformation in a distributed system</H1>
    <B>Werner Thie</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Operating%20on%20metainformation%20in%20a%20distributed%0A%20system&In-Reply-To=%3C520447FA.2070909%40thieprojects.ch%3E"
       TITLE="[Twisted-Python] Operating on metainformation in a distributed system">werner at thieprojects.ch
       </A><BR>
    <I>Thu Aug  8 19:38:02 MDT 2013</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="027306.html">[Twisted-Python] Operating on metainformation in a distributed	system
</A></li>
        <LI>Next message (by thread): <A HREF="059771.html">[Twisted-Python] Operating on metainformation in a distributed	system
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#59770">[ date ]</a>
              <a href="thread.html#59770">[ thread ]</a>
              <a href="subject.html#59770">[ subject ]</a>
              <a href="author.html#59770">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 8/7/13 2:08 PM, Krysk wrote:
&gt;<i> Hello. I tried to ask this on StackOverflow, but Glyph advised me this
</I>&gt;<i> would probably be better. I've had trouble phrasing these questions
</I>&gt;<i> well, so please ask me if you need more information.
</I>&gt;<i>
</I>&gt;<i> I'm working on a game server that needs to support 18,000+ players.
</I>&gt;<i> The game requires fairly intense resource use, and I'd like to have an
</I>&gt;<i> upper limit on the playercount much higher than I need. The solution
</I>&gt;<i> seems obvious: Design a server that can scale out and up as necessary.
</I>&gt;<i>
</I>&gt;<i> Because distributed systems are hard, I tried to simplify the design
</I>&gt;<i> so that isn't a concern as much as possible. To that end, my
</I>&gt;<i> architecure is pretty simple. A player is always assigned to a
</I>&gt;<i> GameHandler instance. By default, a player is assigned to a
</I>&gt;<i> Lobby(GameHandler) instance. They can then queue for a match, and when
</I>&gt;<i> an appropriate match is found the server with the least load creates a
</I>&gt;<i> new handler, say, CaptureTheFlag(GameHandler). Then, the servers which
</I>&gt;<i> those players connected to serve as reverse proxies, forwarding all
</I>&gt;<i> data to the CaptureTheFlag handler. When that game ends, those players
</I>&gt;<i> are all returned to their Lobby(GameHandler) instances. Reverse
</I>&gt;<i> proxies are neccessary because I didn't write the game client and
</I>&gt;<i> modifying it is not an option. Connections cannot be renegotiated. I
</I>&gt;<i> can place all the servers in the same LAN, which should prevent any
</I>&gt;<i> major latency issues, and make bandwidth not a problem.
</I>&gt;<i>
</I>&gt;<i> So far, all is good, I think this design will work well and be very
</I>&gt;<i> simple to work on. However, it raises the big, ugly question: How do I
</I>&gt;<i> share metadata across the distributed nodes?
</I>&gt;<i>
</I>&gt;<i> That's necessary for the matchmaking itself. We might have 400 players
</I>&gt;<i> connected across 10 servers, and we want to make a match where there's
</I>&gt;<i> eight players on one, four on another, and four on another. I also
</I>&gt;<i> need to be able to figure out how many players are on the entire
</I>&gt;<i> network, syncronize bans and configuration data, etc.
</I>&gt;<i>
</I>&gt;<i> I was thinking I could use MySQL to store the configuration data, and
</I>&gt;<i> use Redis for the transient data like who's online, who's in queue,
</I>&gt;<i> etc. Then I could have one server dedicated to operating on all that
</I>&gt;<i> data (such as arranging fair matches). I could use some kind of push
</I>&gt;<i> notification to let servers know when a match has started or ended, or
</I>&gt;<i> just have them query Redis periodically.
</I>&gt;<i>
</I>&gt;<i> This doesn't seem very elegant, easy to work with, or easy to
</I>&gt;<i> implement, so naturally I don't like it very much. I'm sure it will
</I>&gt;<i> work, but I was hoping someone could suggest a more natural approach.
</I>
Hi Krysk

I had similar design constraints when wanting to match up to four human 
players or computer players playing a card game. In the first monolithic 
approach I got a better feeling for how long match making actually 
takes, today we're seeing seldom more than ten tables being in the match 
make process, while there are up to two thousand user playing cards. The 
matchmaking for the game isn't the fun part, so users do away with it 
pretty fast.

After observing user behavior for more than a year, I spread out the 
game logic to separate game servers with a central matchmaking process, 
maintaining all the metadata, doing the load balancing for the game 
servers and handling the broadcasting of status and activity information 
to players. Metadata stored and passed around is the usual stuff like 
game skill level, likeability, friends, blocked users, number of games 
played, and some. The data is kept in a MySQL DB, is fetched at log in 
and passed around with the player instance.

This scheme so far balances very well and in case of needing to handle a 
lot more users, I would separate the matchmaking process to a dedicated 
machine.

The whole setup for more than 50k games played to the end per day (about 
13mins average play time per game) is handled by an 8 core single 
processor machine with 24GB of RAM, usually we do not run more than 5-6 
game logic server processes. The machine is well balanced, extremely 
stable, no runaway situation was observed since deploying the system two 
years ago.

The bottleneck I foresee in our case is the 100MB/s connection we have 
at the hosting center, currently we are only allowed one interface.

For me dodging the sharing of metadata for the matchmaking was crucial, 
I didn't fear the sharing so much as the latency induced by sharing 
metadata among processes or machines, because the added latency adds a 
lot more incongruous stuff happening to the user's experiences. Match 
making on screen with manually selecting partners puts quite a strain on 
the imagination of the average user, with added latency to clicks and 
and answers, the users shy away from match making and start playing 
alone or with the much easier selectable computer players.

HTH, Werner



</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="027306.html">[Twisted-Python] Operating on metainformation in a distributed	system
</A></li>
	<LI>Next message (by thread): <A HREF="059771.html">[Twisted-Python] Operating on metainformation in a distributed	system
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#59770">[ date ]</a>
              <a href="thread.html#59770">[ thread ]</a>
              <a href="subject.html#59770">[ subject ]</a>
              <a href="author.html#59770">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
