<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Twisted-Python] Benchmark of Python WSGI servers
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Benchmark%20of%20Python%20WSGI%20servers&In-Reply-To=%3C20110319013508.2231.1956753728.divmod.xquotient.261%40localhost.localdomain%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="056180.html">
   <LINK REL="Next"  HREF="056187.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Twisted-Python] Benchmark of Python WSGI servers</H1>
    <B>exarkun at twistedmatrix.com</B> 
    <A HREF="mailto:twisted-python%40twistedmatrix.com?Subject=Re%3A%20%5BTwisted-Python%5D%20Benchmark%20of%20Python%20WSGI%20servers&In-Reply-To=%3C20110319013508.2231.1956753728.divmod.xquotient.261%40localhost.localdomain%3E"
       TITLE="[Twisted-Python] Benchmark of Python WSGI servers">exarkun at twistedmatrix.com
       </A><BR>
    <I>Fri Mar 18 19:35:08 MDT 2011</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="056180.html">[Twisted-Python] Benchmark of Python WSGI servers
</A></li>
        <LI>Next message (by thread): <A HREF="056187.html">[Twisted-Python] Benchmark of Python WSGI servers
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#56181">[ date ]</a>
              <a href="thread.html#56181">[ thread ]</a>
              <a href="subject.html#56181">[ subject ]</a>
              <a href="author.html#56181">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 12:54 am, <A HREF="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">glyph at twistedmatrix.com</A> wrote:
&gt;<i>On Mar 18, 2011, at 7:44 PM, Michael Thompson wrote:
</I>&gt;&gt;<i> From the guys who brought you async socket benchmark,
</I>&gt;&gt;<i><A HREF="http://nichol.as/asynchronous-servers-in-python,">http://nichol.as/asynchronous-servers-in-python,</A> comes Python WSGI
</I>&gt;&gt;<i>benchmark
</I>&gt;&gt;<i><A HREF="http://nichol.as/benchmark-of-python-web-servers.">http://nichol.as/benchmark-of-python-web-servers.</A>
</I>&gt;<i>
</I>&gt;<i>Yep, I've seen that before.  It's one of the better benchmarks of its 
</I>&gt;<i>kind in the Python world, but unfortunately stops short of being good 
</I>&gt;:<i>).
</I>&gt;<i>
</I>&gt;<i>The benchmark isn't really saying much that's interesting about WSGI 
</I>&gt;<i>servers anyway.  It mostly says &quot;all of these servers are more than 20x 
</I>&gt;<i>faster than your WSGI app could ever possibly be, if it does anything 
</I>&gt;<i>interesting, so at most the server will account for 5% of your 
</I>&gt;<i>performance&quot;.  The logical conclusion: regardless of what server you're 
</I>&gt;<i>using, go optimize your app first.
</I>&gt;<i>
</I>&gt;<i>While I'd love for Twisted to come out on top of that chart (it's 
</I>&gt;<i>always best to win at things, right?), such an improvement would be of 
</I>&gt;<i>little practical benefit to our users.  First because almost nobody has 
</I>&gt;<i>a WSGI app that is so trivial that it would be significantly helped by 
</I>&gt;<i>speeding up that part of the server, and also the fact that anyone with 
</I>&gt;<i>serious performance requirements in Twisted will be optimizing by 
</I>&gt;<i>calling Resource and Request APIs directly, asynchronously in the main 
</I>&gt;<i>loop (perhaps with multiple processes), not threading WSGI handlers for 
</I>&gt;<i>the critical fast path in their application.  Which, I hasten to remind 
</I>&gt;<i>you, is rarely all of your application.  A performance improvement to 
</I>&gt;<i>static.File, like making it truly non-blocking, would probably be a 
</I>&gt;<i>more significant benefit to most websites that want to be fast than 
</I>&gt;<i>making the thing that calls a WSGI function fast.
</I>&gt;&gt;<i>Is twisted coming out of this so badly because they are using the
</I>&gt;&gt;<i>default reactor, as opposed to epoll?
</I>&gt;<i>
</I>&gt;<i>There isn't really enough analysis to determine why exactly Twisted 
</I>&gt;<i>fares poorly on this particular benchmark.
</I>&gt;<i>
</I>&gt;<i>My pet theory is that it has something to do with transferring data 
</I>&gt;<i>from threads to the I/O loop via queue synchronization, and not being 
</I>&gt;<i>as smart as it could be about buffering, and that particular technique 
</I>&gt;<i>getting slammed really hard for very small request/response pairs.  I 
</I>&gt;<i>hypothesize that more buffering would occur with larger responses with 
</I>&gt;<i>more chunks, and that would bring Twisted's performance up to those of 
</I>&gt;<i>these other servers.
</I>&gt;<i>
</I>&gt;<i>But it's hard to say, and, as I said above the benchmark isn't 
</I>&gt;<i>measuring anything too interesting, so it's hard to work up the 
</I>&gt;<i>motivation to find out.
</I>&gt;&gt;<i>Perhaps the default reactor should be the best available rather than
</I>&gt;&gt;<i>the lowest common denominator.
</I>&gt;<i>
</I>&gt;<i>See &lt;<A HREF="http://twistedmatrix.com/trac/ticket/2234">http://twistedmatrix.com/trac/ticket/2234</A>&gt;.  There should be a 
</I>&gt;<i>ticket for the broader goal too, and maybe it's already filed; I 
</I>&gt;<i>couldn't find it quickly.
</I>
This all seems right on to me.  I just wanted to add that of the &quot;top 
performers&quot;, there is some difference in what's being benchmarked.  Some 
of them use green threads instead of threads.  Some of them are 
multiprocess.  Compared to a thread-based WSGI container, these 
approaches have some performance benefits.  If someone wanted to make 
Twisted WSGI benchmark better, implementing one (or both) of these 
approaches would be one good way to go about it.

A multi-process WSGI container might actually be of practical use, since 
it may make more cores available to your server.  If an application is 
bottlenecked on CPU rather than some high-latency operation (as you can 
only process as many concurrent requests as you have threads in your 
threadpool), more cores can help.

Jean-Paul


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="056180.html">[Twisted-Python] Benchmark of Python WSGI servers
</A></li>
	<LI>Next message (by thread): <A HREF="056187.html">[Twisted-Python] Benchmark of Python WSGI servers
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#56181">[ date ]</a>
              <a href="thread.html#56181">[ thread ]</a>
              <a href="subject.html#56181">[ subject ]</a>
              <a href="author.html#56181">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python">More information about the Twisted-Python
mailing list</a><br>
</body></html>
